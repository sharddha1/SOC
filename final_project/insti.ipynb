{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain faiss-cpu sentence-transformers==2.2.2 InstructorEmbedding pypdf\n",
        "!pip install langchain PyPDF2 faiss-cpu huggingface-hub pandas\n",
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "St4KgzK-4KQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZN4lmkNz8gu"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain-community faiss-cpu\n",
        "!pip install --upgrade --quiet langchain-google-genai\n",
        "!pip install gradio\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "from pypdf import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "import gradio as gr\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYUMP1le0f6c",
        "outputId": "9aa95fb1-5147-4c3f-9178-96627a6392bd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf(file):\n",
        "    document = \"\"\n",
        "\n",
        "    reader = PdfReader(file)\n",
        "    for page in reader.pages:\n",
        "        document += page.extract_text()\n",
        "\n",
        "    return document\n",
        "\n",
        "def read_txt(file):\n",
        "    document = str(file.getvalue())\n",
        "    document = document.replace(\"\\\\n\", \"\\\\n \").replace(\"\\\\r\", \"\\\\r \")\n",
        "\n",
        "    return document\n",
        "\n",
        "# Document Splitting\n",
        "def split_doc(document, chunk_size, chunk_overlap):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    split = splitter.split_text(document)\n",
        "    split = splitter.create_documents(split)\n",
        "    return split"
      ],
      "metadata": {
        "id": "6xy6SWzk0qKn"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def embedding_storing(model_name, split, new_vs_name):\n",
        "#         embeddings = GoogleGenerativeAIEmbeddings(model=model_name)\n",
        "\n",
        "#         db = FAISS.from_documents(split, embeddings)\n",
        "#         db.save_local(\"vector_store/\" + new_vs_name)\n"
      ],
      "metadata": {
        "id": "mY6WyRjb02pO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_rag_llm(\n",
        "    token, llm_model, embeddings_name, temperature, max_length, split, new_vs_name\n",
        "):\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=embeddings_name)\n",
        "\n",
        "    db = FAISS.from_documents(split, embeddings)\n",
        "    db.save_local(\"vector_store/\" + new_vs_name)\n",
        "\n",
        "    # instructor_embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    #     model_name=instruct_embeddings, model_kwargs={\"device\":\"cuda\"}\n",
        "    # )\n",
        "\n",
        "    loaded_db = FAISS.load_local(\n",
        "        f\"vector_store/{new_vs_name}\", embeddings, allow_dangerous_deserialization=True\n",
        "    )\n",
        "\n",
        "    llm = GoogleGenerativeAI(\n",
        "        model=llm_model,\n",
        "        google_api_key=token,\n",
        "        temperature=temperature,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    memory = ConversationBufferWindowMemory(\n",
        "        k=2,\n",
        "        memory_key=\"chat_history\",\n",
        "        output_key=\"answer\",\n",
        "        return_messages=True,\n",
        "    )\n",
        "\n",
        "    qa_conversation = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=loaded_db.as_retriever(),\n",
        "        return_source_documents=True,\n",
        "        memory=memory,\n",
        "    )\n",
        "\n",
        "    return qa_conversation"
      ],
      "metadata": {
        "id": "8MFfpetQ06py"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_file_path = \"/content/drive/MyDrive/FINAL_SCRAPED_DATA.pdf\"\n",
        "document = read_pdf(pdf_file_path)\n",
        "split = split_doc(document, chunk_size=1000, chunk_overlap=200)\n",
        "#embedding_storing(\"models/embedding-001\", split, create_new_vs=True, existing_vector_store=None, new_vs_name=\"insti_vector_store\")\n",
        "\n",
        "llm_model = \"gemini-pro\"\n",
        "instruct_embeddings = \"models/embedding-001\"\n",
        "vector_store_list = \"insti_vector_store\"\n",
        "temperature = 0.7\n",
        "max_length = 512\n",
        "token = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "qa_conversation = prepare_rag_llm(token, llm_model, instruct_embeddings, temperature, max_length, split, new_vs_name=\"insti_vector_store\")\n",
        "\n",
        "# Save the necessary objects for reuse\n",
        "# with open('/content/drive/MyDrive/qa_conversation.pkl', 'wb') as f:\n",
        "#     pickle.dump(qa_conversation, f)"
      ],
      "metadata": {
        "id": "TjE-QE2S24Fn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def test_qa_conversation(qa_conversation, query=\"What is the document above?\"):\n",
        "#   input_dict = {\n",
        "#       \"question\": query,\n",
        "#       \"chat_history\": []\n",
        "#   }\n",
        "#   response = qa_conversation(input_dict)\n",
        "#   return response\n",
        "\n",
        "# response = test_qa_conversation(qa_conversation)\n",
        "\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "yVPTgj38OxuV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_interface(user_input, history):\n",
        "    history = history or []\n",
        "    response = qa_conversation({\"question\": user_input, \"chat_history\": history})\n",
        "    history.append((user_input, response[\"answer\"]))\n",
        "    return history, history"
      ],
      "metadata": {
        "id": "GWnxUScyKtw3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    # Add title\n",
        "    gr.Markdown(\"# InstiGPT\")\n",
        "\n",
        "    # Define the chatbot and state\n",
        "    chatbot = gr.Chatbot()\n",
        "    state = gr.State([])\n",
        "\n",
        "    # Layout for text input and button\n",
        "    with gr.Row():\n",
        "        txt = gr.Textbox(\n",
        "            show_label=False,\n",
        "            placeholder=\"Enter prompt.\",\n",
        "            lines=1,\n",
        "            scale=3\n",
        "        )\n",
        "        btn = gr.Button(\"Send\")  # Enter button\n",
        "\n",
        "    # Configure interaction\n",
        "    txt.submit(chatbot_interface, [txt, state], [chatbot, state])\n",
        "    btn.click(chatbot_interface, [txt, state], [chatbot, state])\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "4ssl_parNmxA",
        "outputId": "26ae326f-65f6-457c-8ff7-da2b9125a977"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://319efb3d57b868481a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://319efb3d57b868481a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}